{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bag of Words (BoW) Model in NLP\n",
        "\n",
        "Trong notebook này, chúng ta sẽ hiện thực mô hình **Bag of Words (BoW)** để biến đổi văn bản thành vector số. \n",
        "\n",
        "Các bước:\n",
        "- Tiền xử lý văn bản\n",
        "- Đếm tần suất từ\n",
        "- Lựa chọn từ phổ biến nhất\n",
        "- Xây dựng ma trận BoW\n",
        "- Trực quan hoá bằng biểu đồ và Word Cloud\n",
        "- Thử ứng dụng trong **Text Classification** (dùng Scikit-learn)\n",
        "\n",
        "Các thư viện sử dụng: `nltk`, `re`, `heapq`, `matplotlib`, `seaborn`, `numpy`, `pandas`, `wordcloud`, `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nltk matplotlib seaborn wordcloud pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import heapq\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Preprocessing the Text\n",
        "Ở đây ta lấy một đoạn văn bản mẫu và tiền xử lý để phục vụ cho BoW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"Beans. I was trying to explain to somebody as we were flying in, that's corn. That's beans. \n",
        "And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again \n",
        "for that outstanding introduction. I have a bunch of good friends here today, including somebody who \n",
        "I served with, who is one of the finest senators in the country and we're lucky to have him, your Senator, \n",
        "Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven't seen in a long time \n",
        "and somehow he has not aged and I have. And it's great to see you, Governor. I want to thank President Killeen \n",
        "and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at \n",
        "the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public \n",
        "service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still \n",
        "wondering why I didn't speak at the commencement.\"\"\"\n",
        "\n",
        "dataset = nltk.sent_tokenize(text)\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    dataset[i] = dataset[i].lower()\n",
        "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
        "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n",
        "\n",
        "for i, sentence in enumerate(dataset):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Counting Word Frequencies\n",
        "Ta đếm tần suất từ và loại bỏ stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2count = {}\n",
        "for data in dataset:\n",
        "    words = nltk.word_tokenize(data)\n",
        "    for word in words:\n",
        "        word2count[word] = word2count.get(word, 0) + 1\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_word2count = {w: c for w, c in word2count.items() if w not in stop_words}\n",
        "\n",
        "word_freq_df = pd.DataFrame(list(filtered_word2count.items()), columns=['Word','Frequency']).sort_values(by='Frequency', ascending=False)\n",
        "word_freq_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Selecting Top Frequent Words\n",
        "Chọn ra 10 từ phổ biến nhất và hiển thị biểu đồ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_words = heapq.nlargest(10, filtered_word2count, key=filtered_word2count.get)\n",
        "print(\"Top 10 frequent words:\", freq_words)\n",
        "\n",
        "top_words = sorted(filtered_word2count.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "words, counts = zip(*top_words)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(words, counts, color='skyblue')\n",
        "plt.title('Top 10 Most Frequent Words')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Building BoW Matrix\n",
        "Ma trận BoW nhị phân cho từng câu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = []\n",
        "for data in dataset:\n",
        "    vector = [1 if word in nltk.word_tokenize(data) else 0 for word in freq_words]\n",
        "    X.append(vector)\n",
        "X = np.asarray(X)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(X, cmap='RdYlGn', cbar=False, annot=True, fmt=\"d\", xticklabels=freq_words,\n",
        "            yticklabels=[f\"Sentence {i+1}\" for i in range(len(dataset))])\n",
        "plt.title('Bag of Words Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Word Cloud Visualization\n",
        "Word Cloud thể hiện trực quan độ phổ biến của từ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(filtered_word2count)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Application - Text Classification\n",
        "Áp dụng BoW để phân loại văn bản đơn giản bằng **Naive Bayes**.\n",
        "\n",
        "Ở đây ta tạo một dataset nhỏ với 2 nhãn: *positive* và *negative*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    'I love this movie, it was fantastic and wonderful',\n",
        "    'This film was horrible and boring',\n",
        "    'I enjoyed the storyline and the acting',\n",
        "    'I dislike this movie, it was too slow',\n",
        "    'Amazing experience, would watch again',\n",
        "    'Terrible film, waste of time'\n",
        "]\n",
        "\n",
        "labels = ['positive','negative','positive','negative','positive','negative']\n",
        "\n",
        "# Vector hóa bằng CountVectorizer (BoW)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kết luận\n",
        "Mô hình BoW có thể được dùng để biểu diễn văn bản cho nhiều tác vụ NLP như:\n",
        "- **Text Classification** (như ví dụ)\n",
        "- **Sentiment Analysis**\n",
        "- **Document Clustering**\n",
        "\n",
        "Tuy nhiên, nó không giữ ngữ cảnh, nên khi cần ngữ nghĩa và mối quan hệ từ, ta nên dùng các mô hình khác như **Word2Vec**, **TF-IDF**, **BERT**,..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}