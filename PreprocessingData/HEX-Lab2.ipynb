{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFD7DVZ-xKdT"
   },
   "source": [
    "# Homework Lab 2: Text Preprocessing with Vietnamese\n",
    "**Overview:** In this exercise, we will build a text preprocessing program for Vietnamese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOAeiqdrxKdt"
   },
   "source": [
    "Import the necessary libraries. Note that we are using the underthesea library for Vietnamese tokenization. To install it, follow the instructions below. ([link](https://github.com/undertheseanlp/underthesea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RrFQ_Ht_xKdu"
   },
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import codecs\n",
    "import sys\n",
    "import re\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC27lBQZxKdw"
   },
   "source": [
    "## Question 1: Create a Corpus and Survey the Data\n",
    "\n",
    "The data in this section is partially extracted from the [VNTC](https://github.com/duyvuleo/VNTC) dataset. VNTC is a Vietnamese news dataset covering various topics. In this section, we will only process the science topic from VNTC. We will create a corpus from both the train and test directories. Complete the following program:\n",
    "\n",
    "- Write `sentences_list` to a file named `dataset_name.txt`, with each element as a document on a separate line.\n",
    "- Check how many documents are in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GyNKT8wAxKdx",
    "outputId": "b2eb7c10-da8d-49cb-8b7d-4f6543700cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels = test labels\n",
      "Number of documents in corpus (VNTC_khoahoc): 3916\n",
      "Corpus saved to VNTC_khoahoc.txt\n",
      "\n",
      "Sample documents:\n",
      "Document 1: Ninh Thuận: Địa điểm ưu tiên đặt nhà máy điện hạt nhân Một góc Ninh Thuận, địa điểm ưu tiên lựa chọn đặt nhà máy điện hạt nhân Ông Vương Hữu Tấn, Viện trưởng Viện Năng Lượng Nguyên tử Việt Nam cho biế...\n",
      "\n",
      "Document 2: Công nghệ nuôi tạo ngọc trai đen Viên ngọc trai đen to bằng hạt nhãn, có kích thước 15mm do ông Thiện nuôi cấy thường có giá từ 2.000 USD trở lên Trên thị trường những năm gần đây, những viên ngọc tra...\n",
      "\n",
      "Document 3: Hơn 16.000 loài có nguy cơ tuyệt chủng Loài vật đẹp đẽ này có thể biến mất hoàn toàn khỏi địa cầu trong tương lai Hiệp hội Bảo tồn thế giới (IUCN) vừa đưa ra Danh sách đỏ năm 2006, trong đó gấu Bắc cự...\n",
      "\n",
      "Document 4: Nghiên cứu thành công thiết bị đun nước bằng năng lượng mặt trời Thiết bị đun nước bằng năng lượng mặt trời. Sau gần 1 năm thực hiện, đề tài nghiên cứu khoa học “ứng dụng công nghệ đun nước nóng sinh ...\n",
      "\n",
      "Document 5: Tạo bộ kit phát hiện vi khuẩn bệnh thương hàn (NLĐ)- Thạc sĩ Phạm Thái Bình, Trung tâm Phát triển khoa học và công nghệ trẻ, vừa nghiên cứu chế tạo thành công bộ kit phát hiện vi khuẩn bệnh thương hàn...\n",
      "\n",
      "Document 6: Công nghệ LED giúp màn hình sáng và tiết kiệm điện Chất lượng hình ảnh trên màn hiển thị tinh thể lỏng, máy tính xách tay, TV và các thiết bị điện tử khác sẽ được nâng lên đáng kể với công nghệ chiếu ...\n",
      "\n",
      "Document 7: Sếu đầu đỏ sẽ biến mất? Sếu đầu đỏ ở vườn quốc gia Tràm Chim Vườn quốc gia Tràm Chim từng là nơi ở quan trọng nhất của sếu đầu đỏ ở VN. Số lượng sếu về Tràm Chim trong những năm 1980 thường không dưới...\n",
      "\n",
      "Document 8: Trung Quốc: Phóng vệ tinh nghiên cứu khoa học Sáng 27-4, Trung Quốc đã phóng thành công vệ tinh nghiên cứu vào quỹ đạo, đây là lần phóng vệ tinh đầu tiên của Trung Quốc trong năm nay.  Vệ tinh nghiên ...\n",
      "\n",
      "Document 9: Tôi đã dịch Trịnh Xuân Thuận như thế nào? GS-TS Trịnh Xuân Thuận nói chuyện về vật lý thiên văn với sinh viên Trường ĐH Dân lập Ngoại ngữ - Tin học TPHCM, ngày 15-7-2002. Ảnh: H. Thúy Tôi làm quen với...\n",
      "\n",
      "Document 10: Người Vượn ở Malaysia Tấm ảnh Bigfoot này trích từ một cuốn phim của Roger Patterson quay ở Bluff Creek, bang California, năm 1967 Mặc dù cho đến nay chưa ai bắt sống hay tìm thấy xác Bigfoot (Chân To...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"VNTC_khoahoc\"\n",
    "path = ['./data/VNTC_khoahoc/Train_Full/', './data/VNTC_khoahoc/Test_Full/']\n",
    "\n",
    "if os.listdir(path[0]) == os.listdir(path[1]):\n",
    "    folder_list = [os.listdir(path[0]), os.listdir(path[1])]\n",
    "    print(\"train labels = test labels\")\n",
    "else:\n",
    "    print(\"train labels differ from test labels\")\n",
    "\n",
    "doc_num = 0\n",
    "sentences_list = []\n",
    "meta_data_list = []\n",
    "for i in range(2):\n",
    "    for folder_name in folder_list[i]:\n",
    "        folder_path = path[i] + folder_name\n",
    "        if folder_name[0] != \".\":\n",
    "            for file_name in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "                # Read the file content into f\n",
    "                f = codecs.open(file_name, 'br')\n",
    "                # Convert the data to UTF-16 format for Vietnamese text\n",
    "                file_content = (f.read().decode(\"utf-16\")).replace(\"\\r\\n\", \" \")\n",
    "                sentences_list.append(file_content.strip())\n",
    "                f.close\n",
    "                # Count the number of documents\n",
    "                doc_num += 1\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "output_file = f\"{dataset_name}.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for document in sentences_list:\n",
    "        f.write(document + \"\\n\")\n",
    "\n",
    "print(f\"Number of documents in corpus ({dataset_name}): {doc_num}\")\n",
    "print(f\"Corpus saved to {output_file}\")\n",
    "\n",
    "print(\"\\nSample documents:\")\n",
    "for i, doc in enumerate(sentences_list[:10], 1):\n",
    "    print(f\"Document {i}: {doc[:200]}...\\n\")\n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Write Preprocessing Functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KXHcDpuxKd0"
   },
   "source": [
    "### Question 2.1: Write a Function to Clean Text\n",
    "Hint:\n",
    "- The text should only retain the following characters: aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\\n",
    "- Then trim the whitespace in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8hIglDXxKd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xin chào!!! Tôi là AI??? 123 ok nè\n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    #### YOUR CODE HERE ####\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "    \n",
    "    allowed_chars = r\"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬ\" \\\n",
    "                    r\"bBcCdDđĐ\" \\\n",
    "                    r\"eEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆ\" \\\n",
    "                    r\"fFgGhHiIìÌỉỈĩĨíÍịỊ\" \\\n",
    "                    r\"jJkKlLmMnNoOòÒỏỎõÕóÓọỌ\" \\\n",
    "                    r\"ôÔồỒổỔỗỖốỐộỘ\" \\\n",
    "                    r\"ơƠờỜởỞỡỠớỚợỢ\" \\\n",
    "                    r\"pPqQrRsStTuUùÙủỦũŨúÚụỤ\" \\\n",
    "                    r\"ưƯừỪửỬữỮứỨựỰ\" \\\n",
    "                    r\"vVwWxXyYỳỲỷỶỹỸýÝỵỴzZ\" \\\n",
    "                    r\"0-9\\(\\),!?'\\\\\"\n",
    "    # Reduce the unallowed chars\n",
    "    text = re.sub(f\"[^{allowed_chars}]\", \" \", string)\n",
    "    # Reduce the space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "    #### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KfXstqAxKd1"
   },
   "source": [
    "### Question 2.2: Write a Function to Convert Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRwgVjxhxKd1"
   },
   "outputs": [],
   "source": [
    "# make all text lowercase\n",
    "def text_lowercase(string):\n",
    "    #### YOUR CODE HERE ####\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "    \n",
    "    return string.lower()\n",
    "    #### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYM_GO_5xKd2"
   },
   "source": [
    "### Question 2.3: Tokenize Words\n",
    "Hint: Use the `word_tokenize()` function imported above with two parameters: `strings` and `format=\"text\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pty34NwyxKd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Học_sinh đang học bài ở trường .\n"
     ]
    }
   ],
   "source": [
    "def tokenize(strings):\n",
    "    #### YOUR CODE HERE ####\n",
    "    if strings is None:\n",
    "        return []\n",
    "    \n",
    "    return word_tokenize(strings, format=\"text\")\n",
    "    #### END YOUR CODE #####\n",
    "print(tokenize(\"Học sinh đang học bài ở trường.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gQGmL4gxKd2"
   },
   "source": [
    "### Question 2.4: Remove Stop Words\n",
    "To remove stop words, we use a list of Vietnamese stop words stored in the file `./vietnamese-stopwords.txt`. Complete the following program:\n",
    "- Check each word in the text (`strings`). If a word is not in the stop words list, add it to `doc_words`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqStv2rPxKd3"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(strings):\n",
    "    #### YOUR CODE HERE ####\n",
    "    pass\n",
    "    #### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUNOKigIxKd4"
   },
   "source": [
    "## Question 2.5: Build a Preprocessing Function\n",
    "Hint: Call the functions `clean_str`, `text_lowercase`, `tokenize`, and `remove_stopwords` in order, then return the result from the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vd-el91xKd_"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(strings):\n",
    "    #### YOUR CODE HERE ####\n",
    "    pass\n",
    "    #### END YOUR CODE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BGOqa1mxKeA"
   },
   "source": [
    "## Question 3: Perform Preprocessing\n",
    "Now, we will read the corpus from the file created in Question 1. After that, we will call the preprocessing function for each document in the corpus.\n",
    "\n",
    "Hint: Call the `text_preprocessing()` function with `doc_content` as the input parameter and save the result in the variable `temp1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "clean_docs = []\n",
    "#### END YOUR CODE #####\n",
    "print(\"\\nlength of clean_docs = \", len(clean_docs))\n",
    "print('clean_docs[0]:\\n' + clean_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFhai6BwxKeB"
   },
   "source": [
    "## Question 4: Save Preprocessed Data\n",
    "Hint: Save the preprocessed data to a file named `dataset_name + '.clean.txt'`, where each document is written on a separate line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfHmSiRrxKeB"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "#### YOUR CODE HERE ####"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vietnamnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
